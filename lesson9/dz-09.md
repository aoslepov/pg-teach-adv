### Домашнее задание. Разворачиваем и настраиваем БД с большими данными  

#### Цель:
- знать различные механизмы загрузки данных  
- уметь пользоваться различными механизмами загрузки данных  

#### Описание/Пошаговая инструкция выполнения домашнего задания:  
- Необходимо провести сравнение скорости работы запросов на различных СУБД  

Выбрать одну из СУБД  
Загрузить в неё данные (от 10 до 100 Гб)  
Сравнить скорость выполнения запросов на PosgreSQL и выбранной СУБД 

Разворачиваем ВМ в яндекс облаке  
```
yc compute instance create \
  --name pg-teach-01 \
  --hostname pg-teach-01 \
  --create-boot-disk size=50G,type=network-ssd,image-folder-id=standard-images,image-family=ubuntu-2204-lts \
  --cores 4 \
  --memory 8G \
  --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 \
  --zone ru-central1-a \
  --metadata-from-file ssh-keys=/home/aslepov/meta.txt
```

#### заливка данных в postgres

Устанавливаем postgres
```
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt-get update
sudo apt-get -y install postgresql-15
```

Настраиваем в postgres.auto.conf конфиг перед заливкой и перегружаем экземпляр
```
transaction_isolation = 'read uncommitted'
default_transaction_isolation = 'read uncommitted'
wal_level = minimal
max_wal_senders = 0
max_wal_size = '10 GB'
min_wal_size = '512 MB'
fsync = off
full_page_writes=off
checkpoint_timeout = '15 min'
checkpoint_completion_target = 0.9
wal_compression = off
synchronous_commit = off
max_worker_processes = 4
max_parallel_workers_per_gather = 4
max_parallel_maintenance_workers = 4
max_parallel_workers = 4
parallel_leader_participation = on
autovacuum = off
shared_buffers = '2730 MB'
work_mem = '38 MB'
maintenance_work_mem = '409 MB'
effective_cache_size = '6553 MB'
effective_io_concurrency = 200
random_page_cost = 1.2

```

 
Создаём таблицу
```
create  table chicago_taxi (
taxi_id bigint,
trip_start_timestamp TIMESTAMP,
trip_end_timestamp TIMESTAMP,
trip_seconds bigint,
trip_miles numeric,
pickup_census_tract bigint,
dropoff_census_tract bigint,
pickup_community_area bigint,
dropoff_community_area bigint,
fare numeric,
tips numeric,
tolls numeric,
extras numeric,
trip_total numeric,
payment_type text,
company text,
pickup_latitude numeric,
pickup_longitude numeric,
dropoff_latitude numeric,
dropoff_longitude numeric
);
```

Скрипт для заливки будем запускать 2 раза для заливки ~12GB данных  
```
for i in $(ls -1 /load/chicago*.csv); do
	echo $i
	psql -d postgres -c "
BEGIN;
COPY chicago_taxi(taxi_id,trip_start_timestamp,trip_end_timestamp,trip_seconds,trip_miles,pickup_census_tract,dropoff_census_tract,pickup_community_area,dropoff_community_area,fare,tips,tolls,extras,trip_total,payment_type,company,pickup_latitude,pickup_longitude,dropoff_latitude,dropoff_longitude)
FROM '$i'
DELIMITER ','
CSV HEADER;
commit; " &
done
```


Мониторим заливку  
```
postgres=# select * from pg_stat_progress_copy;
 pid  | datid | datname  | relid |  command  | type | bytes_processed | bytes_total | tuples_processed | tuples_excluded
------+-------+----------+-------+-----------+------+-----------------+-------------+------------------+-----------------
 8471 |     5 | postgres | 16393 | COPY FROM | FILE |         6619136 |   184423109 |            60635 |               0
 8472 |     5 | postgres | 16393 | COPY FROM | FILE |         7864320 |   189657705 |            72114 |               0
 8474 |     5 | postgres | 16393 | COPY FROM | FILE |         7012352 |   210509642 |            63882 |               0
 8473 |     5 | postgres | 16393 | COPY FROM | FILE |         7995392 |   142068451 |            72362 |               0
 8478 |     5 | postgres | 16393 | COPY FROM | FILE |         6094848 |   144370403 |            54607 |               0
 8475 |     5 | postgres | 16393 | COPY FROM | FILE |         6488064 |   138280907 |            58427 |               0
 8476 |     5 | postgres | 16393 | COPY FROM | FILE |         7602176 |   167544420 |            67557 |               0
 8479 |     5 | postgres | 16393 | COPY FROM | FILE |         9306112 |   211196193 |            85252 |               0
 8477 |     5 | postgres | 16393 | COPY FROM | FILE |         7274496 |   214139224 |            66557 |               0
 8482 |     5 | postgres | 16393 | COPY FROM | FILE |         8060928 |   166738011 |            73195 |               0
 8483 |     5 | postgres | 16393 | COPY FROM | FILE |         7340032 |   190262071 |            66808 |               0
 8480 |     5 | postgres | 16393 | COPY FROM | FILE |         7667712 |   211736753 |            70201 |               0
```

Время заливки ~ 8мин  
  
После заливки запускаем вакуум аналайз и создаём индексы
```
vacuum analyze chicago_taxi;
create index concurrently idx_taxi_id on chicago_taxi(taxi_id);
create index concurrently idx_dates on chicago_taxi(trip_start_timestamp,trip_end_timestamp);
```

Оставляем в postgres.auto.conf следующие записи и перегружаем инстанс постгрес
```
checkpoint_timeout = '15 min'
checkpoint_completion_target = 0.9
max_worker_processes = 4
max_parallel_workers_per_gather = 4
max_parallel_maintenance_workers = 4
max_parallel_workers = 4
parallel_leader_participation = on
shared_buffers = '2730 MB'
work_mem = '38 MB'
maintenance_work_mem = '409 MB'
effective_cache_size = '6553 MB'
effective_io_concurrency = 200
random_page_cost = 1.2
```

Смотрим время выполнения запросов
```

-- выборка рандомной записи по индексу
select taxi_id from chicago_taxi order by random() limit 1;
Time: 5399.152 ms (00:05.399)


--выборка данных за неделю
postgres=# select taxi_id,trip_start_timestamp,trip_end_timestamp from chicago_taxi where trip_start_timestamp between date'2016-02-01' and date'2016-02-07';
Time: 5058.900 ms (00:05.058)


--выборка данных за месяц
select taxi_id,trip_start_timestamp,trip_end_timestamp from chicago_taxi where trip_start_timestamp between date'2016-02-01' and date'2016-02-27';
Time: 2.48 min (02.48.557)

```


#### заливка данных в кликхаус

Устанавливаем кликхаус  
```
sudo apt-get install -y apt-transport-https ca-certificates dirmngr
GNUPGHOME=$(mktemp -d)
sudo GNUPGHOME="$GNUPGHOME" gpg --no-default-keyring --keyring /usr/share/keyrings/clickhouse-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 8919F6BD2B48D754
sudo rm -r "$GNUPGHOME"
sudo chmod +r /usr/share/keyrings/clickhouse-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/clickhouse-keyring.gpg] https://packages.clickhouse.com/deb stable main" | sudo tee \
    /etc/apt/sources.list.d/clickhouse.list
sudo apt-get update
sudo apt-get install -y clickhouse-server clickhouse-client
```

Создаём талицу  
```
create or replace  table chicago_taxi (
taxi_id UInt64,
trip_start_timestamp DateTime,
trip_end_timestamp DateTime,
trip_seconds UInt64,
trip_miles Float32,
pickup_census_tract UInt64,
dropoff_census_tract UInt64,
pickup_community_area UInt64,
dropoff_community_area UInt64,
fare Float32,
tips Float32,
tolls Float32,
extras Float32,
trip_total Float32,
payment_type text,
company text,
pickup_latitude Float32,
pickup_longitude Float32,
dropoff_latitude Float32,
dropoff_longitude Float32
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(trip_start_timestamp)
PRIMARY KEY (trip_start_timestamp,trip_end_timestamp, taxi_id)
ORDER BY (trip_start_timestamp,trip_end_timestamp, taxi_id)
;
```


Скрипт заливки в кликхаус (также запускаем 2 раза)
```
for i in $(ls -1 /load/chicago*.csv); do
        echo $i
	cat $i | clickhouse-client --multiquery  --password=testload --query="INSERT INTO chicago_taxi FORMAT CSVWithNames " &
done
```


Результаты заливки  
```
Заливка ~ 30 сек 


pg-teach-01.ru-central1.internal :) optimize table chicago_taxi final;

-- выборка рандомной записи
pg-teach-01.ru-central1.internal :) select taxi_id from chicago_taxi order by rand() limit 1;

Query id: e017b4ea-3352-496c-94ae-019b7ce80e3b

┌─taxi_id─┐
│     168 │
└─────────┘

1 row in set. Elapsed: 0.142 sec.


-- выборка за неделю
select taxi_id,trip_start_timestamp,trip_end_timestamp from chicago_taxi where trip_start_timestamp between '2016-02-01' and '2016-02-07';
677922 rows in set. Elapsed: 0.603 sec. Processed 679.94 thousand rows, 75.53 MB (1.13 million rows/s., 125.36 MB/s.)

-- выборка за месяц
select taxi_id,trip_start_timestamp,trip_end_timestamp from chicago_taxi where trip_start_timestamp between '2016-02-01' and '2016-02-27';
3167190 rows in set. Elapsed: 0.829 sec. Processed 3.17 million rows, 352.17 MB (3.83 million rows/s., 425.02 MB/s.)

```
